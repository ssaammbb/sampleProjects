{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Activation_Functions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrvAKIjj1cOwkaH8uO/+u3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjHR3rW_8ivl","executionInfo":{"status":"ok","timestamp":1642371216628,"user_tz":420,"elapsed":4132,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}},"outputId":"0d7b61ae-5d16-4284-ceba-0c3c9000551d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["import os, sys\n","import subprocess\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"import_ipynb\"])\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import import_ipynb\n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/net')\n","\n","import numpy as np\n","from Function_Wrapper import *\n","\n","class activation_function:\n","  def __new__(self, z = 0, is_deriv = False, function = \"\"):\n","    z = np.asarray(z)\n","    implemented_functions = [\"ReLU\", \"tanh\", \"sigmoid\", \"identity\"], \"softmax\"\n","    if(function == \"ReLU\"):\n","      if(is_deriv):\n","        return 1 * (z > 0) \n","      return z * (z > 0)\n","    elif(function == \"tanh\"):\n","      if(is_deriv):\n","        return 1-np.tanh(z)**2\n","      return np.tanh(z)\n","    elif(function == \"sigmoid\"):\n","      if(is_deriv):\n","        #return (1.0/(1 + np.exp(-z)) * (1- (1.0/(1 + np.exp(-z)))))\n","        return (z * (1 - z))\n","      return 1.0/(1 + np.exp(-z))\n","    elif(function == \"identity\"):\n","      if(is_deriv):\n","        return np.full(np.shape(z), 1)\n","      return z\n","    elif(function == \"softmax\"):\n","      z = np.reshape(z, (1, len(z)))\n","      tmp = z - z.max(axis=1)[:, np.newaxis]\n","      z = np.exp(tmp)\n","      z = z/(z.sum(axis=1)[:, np.newaxis])\n","      return z\n","    else:\n","      raise NotImplementedError(\"Activation function not implemented, currently implemented functions are:\" + str(implemented_functions))\n","\n","ReLU = function_wrapper(activation_function, is_deriv = False, function = \"ReLU\")\n","ReLU_prime = function_wrapper(activation_function, is_deriv = True, function = \"ReLU\")\n","tanh = function_wrapper(activation_function, is_deriv = False, function = \"tanh\")\n","tanh_prime = function_wrapper(activation_function, is_deriv = True, function = \"tanh\")\n","sigmoid = function_wrapper(activation_function, is_deriv = False, function = \"sigmoid\")\n","sigmoid_prime = function_wrapper(activation_function, is_deriv = True, function = \"sigmoid\")\n","identity = function_wrapper(activation_function, is_deriv = False, function = \"identity\")\n","identity_prime = function_wrapper(activation_function, is_deriv = True, function = \"identity\")\n","softmax = function_wrapper(activation_function, function = \"softmax\")"]}]}