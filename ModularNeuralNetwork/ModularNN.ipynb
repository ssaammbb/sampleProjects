{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ModularNN.ipynb","provenance":[{"file_id":"12SxU32YG2QohH-ydIH2A5igCK-k2A2G3","timestamp":1642117850394},{"file_id":"1kdGFb6r0e7ar4jlPNYe7o-5GKZqcARxZ","timestamp":1642026595688},{"file_id":"19VDo1LUY_412hODs4ScsEp_1ngdRk4CQ","timestamp":1641872618841},{"file_id":"16aCnL9tbZzwi45MsMduFo6NZqcmCBd_J","timestamp":1641870705672},{"file_id":"1M1qEPaB4phtSgwvm2O6jVpAnjPAJWqsJ","timestamp":1641078700335},{"file_id":"1UysxDi8UnTmeeESdTeLO-tnQJhXdHDWU","timestamp":1640766469677}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP8TUDW+sk+ru2CCkAUWksG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os, sys\n","import subprocess\n","\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"import_ipynb\"])\n","import import_ipynb\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/ModularNeuralNetwork')\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from Function_Wrapper import function_wrapper\n","from Activation_Functions import ReLU, ReLU_prime, tanh, tanh_prime, sigmoid, sigmoid_prime, identity, identity_prime, softmax\n","from Cost_Functions import mse, mse_prime, cross_entropy, cross_entropy_prime\n","from Initialization_Strategies import rand, normal, Xavier, He, none"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slCF-HBwZl6O","executionInfo":{"status":"ok","timestamp":1644620353829,"user_tz":420,"elapsed":20391,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}},"outputId":"e714ce93-47d3-465e-bda0-890e377575bf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","importing Jupyter notebook from Function_Wrapper.ipynb\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","importing Jupyter notebook from Activation_Functions.ipynb\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","importing Jupyter notebook from Cost_Functions.ipynb\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","importing Jupyter notebook from Initialization_Strategies.ipynb\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def adam_variations(layer, index, adam_variation = \"Adam\"):\n","  ###\n","  # Function to implement Adam, Adamax, and Nadam solvers\n","  #\n","  # Parameters:\n","  # layer- Current layer to update weights and biases\n","  # index- Index of the current batch\n","  # adam_variation- Choice between Adam, Adamax, and Nadam\n","  ###\n","\n","\n","  dW = layer.weights_gradient # Variables to store the weights and biases gradients found in backpropagation\n","  dB = layer.biases_gradient\n","\n","  layer.adam_parameters.m_weights = layer.adam_parameters.beta_0 * layer.adam_parameters.m_weights + (1 - layer.adam_parameters.beta_0) * dW # Update m values for weights\n","  \n","  layer.adam_parameters.m_biases = layer.adam_parameters.m_biases * layer.adam_parameters.beta_0 # Update m values for biases\n","  layer.adam_parameters.m_biases = layer.adam_parameters.m_biases + ((1 - layer.adam_parameters.beta_0) * dB)\n","\n","  m_hat_weights = layer.adam_parameters.m_weights / (1 - np.power(layer.adam_parameters.beta_0, index)) # find m_hat for weights and biases\n","  m_hat_biases = layer.adam_parameters.m_biases / (1 - np.power(layer.adam_parameters.beta_0, index))\n","\n","  if(adam_variation == \"Adam\" or adam_variation == \"Nadam\"): # Adam and Nadam share more operations, so they are both included here\n","    layer.adam_parameters.v_weights = layer.adam_parameters.beta_1 * layer.adam_parameters.v_weights + (1 - layer.adam_parameters.beta_1) * np.power(dW, 2) # Calculate v values\n","    layer.adam_parameters.v_biases = layer.adam_parameters.beta_1 * layer.adam_parameters.v_biases + (1 - layer.adam_parameters.beta_1) * dB\n","    \n","    if(adam_variation == \"Nadam\"): # Additional update to m_hat if Nadam is being used\n","      m_hat_weights += (1 - layer.adam_parameters.beta_0) * dW / (1 - np.power(layer.adam_parameters.beta_0, index))\n","      m_hat_biases += (1 - layer.adam_parameters.beta_0) * dB / (1 - np.power(layer.adam_parameters.beta_0, index)) \n","\n","    v_hat_weights = layer.adam_parameters.v_weights / (1 - np.power(layer.adam_parameters.beta_1, index)) # Calculate v_hat for weights and biases\n","    v_hat_biases = layer.adam_parameters.v_biases / (1 - np.power(layer.adam_parameters.beta_1, index))\n","    \n","    layer.weights -= layer.eta * (m_hat_weights / (np.sqrt(v_hat_weights) + layer.adam_parameters.epsilon)) # Update weights and biases\n","    layer.biases -= layer.eta * (m_hat_biases / (np.sqrt(v_hat_biases) + layer.adam_parameters.epsilon))\n","\n","  elif(adam_variation == \"Adamax\"):\n","    layer.adam_parameters.v_weights = np.maximum(layer.adam_parameters.beta_1 * layer.adam_parameters.v_weights, np.abs(dW))\n","    layer.adam_parameters.v_biases = np.maximum(layer.adam_parameters.beta_1 * layer.adam_parameters.v_biases, np.abs(dB)) \n","    \n","    layer.weights = layer.weights - (layer.eta * (m_hat_weights / (layer.adam_parameters.v_weights + layer.adam_parameters.epsilon)))\n","    layer.biases = layer.biases - (layer.eta * (m_hat_biases / (layer.adam_parameters.v_biases + layer.adam_parameters.epsilon)))\n","\n","  else:\n","    raise Exception(\"Given value is not Adam or a variation thereof\")\n","  \n","# Presets for each variation, made with function_wrapper function in Function_Wrapper.ipynb  \n","standard_Adam = function_wrapper(adam_variations, adam_variation = \"Adam\")\n","Nadam = function_wrapper(adam_variations, adam_variation = \"Nadam\")\n","Adamax = function_wrapper(adam_variations, adam_variation = \"Adamax\")"],"metadata":{"id":"cde-FDiYTbMm","executionInfo":{"status":"ok","timestamp":1644620353831,"user_tz":420,"elapsed":19,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def solver(input_layer, X, y, epochs, batch_proportion = .5, solver_type = \"SGD\", adam_variation = None):\n","  ###\n","  # Function to implement various optimizers. Includes gradient descent, stochastic gradient descent, and variations of Adam\n","  #\n","  # Parameters:\n","  # input_layer- Input layer of the network\n","  # X- Feature set from the current batch\n","  # y- Classifications for the current batch\n","  # batch_proportion- Proportion of total training data made up by the current batch\n","  # solver_type- Standard, SGD, or Adam. If Adam is chosen, the computation will be handeled by the adam_variations function regardless of variation choice\n","  # adam_variation- Adam, Adamax, or Nadam\n","  ###\n","\n","  implemented_solvers = [\"SGD\", \"standard\", \"Adam\", \"Nadam\", \"Adamax\"] # List of implemented optimizers\n","  output_layer = input_layer.get_output_layer() # find the output layer of the network given the input layer\n","\n","  if(solver_type == \"SGD\"): # SGD calculation\n","\n","    num_samples = len(y)\n","    batch_size = int(batch_proportion * num_samples) # Calculate size of batch\n","\n","    for index in range(epochs):\n","      indices = np.random.choice(num_samples, size = batch_size) # Find random subset of training data of size batch_size\n","      subset_X = X[indices]; subset_y = y[indices]\n","\n","      output_layer.batch_backward_propagate(subset_X, subset_y, index) # Backpropagate for current batch\n","      output_layer.update_parameters() # Update weights and biases \n","      \n","  elif(solver_type == \"Adam\"): # Set up Adam calculation, delegate computation of final values to adam_variations\n","\n","    num_samples = len(y)\n","    batch_size = int(batch_proportion * num_samples) # Calculate size of batch \n","\n","    for index in range(epochs): \n","      indices = np.random.choice(num_samples, size = batch_size) # Find random subset of training data of size batch_size\n","      subset_X = X[indices]; subset_y = y[indices]\n","\n","      output_layer.batch_backward_propagate(subset_X, subset_y, index) # Backpropagate\n","\n","      layer = output_layer\n","      while(layer.prev_layer != None): # Perform the chosen Adam variation on each layer\n","        adam_variation(layer, index + 1)\n","        layer = layer.prev_layer\n","\n","  elif(solver_type == \"standard\"): # Standard gradient descent\n","\n","    for index in range(epochs):\n","      for X_i, y_i in zip(X, y):\n","        output_layer.batch_backward_propagate(X_i, y_i, index)\n","        output_layer.update_parameters()\n","    \n","  else:\n","    raise NotImplementedError(\"Solver not implemented, currently implemented functions are:\" + str(implemented_solvers))\n","\n","# Presets for solvers\n","standard_solver = function_wrapper(solver, solver_type = \"standard\")\n","SGD_solver = function_wrapper(solver, solver_type = \"SGD\")\n","Adam_solver = function_wrapper(solver, solver_type = \"Adam\", adam_variation = standard_Adam)\n","Nadam_solver = function_wrapper(solver, solver_type = \"Adam\", adam_variation = Nadam)\n","Adamax_solver = function_wrapper(solver, solver_type = \"Adam\", adam_variation = Adamax)"],"metadata":{"id":"qOXtVLtQM8Bg","executionInfo":{"status":"ok","timestamp":1644620353832,"user_tz":420,"elapsed":16,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class Adam_Parameters: # Class to store parameters of Adam calculations\n","  def __init__(self, layer, beta_0 = .9, beta_1 = .999, epsilon = .0001):\n","    self.layer = layer\n","    self.weights_shape = np.shape(self.layer.weights)\n","    self.biases_shape = np.shape(self.layer.biases)\n","\n","    self.m_weights = np.zeros(self.weights_shape)\n","    self.v_weights = self.m_weights\n","    self.m_biases = np.zeros(self.biases_shape)\n","    self.v_biases = self.m_biases\n","\n","    self.beta_0 = beta_0\n","    self.beta_1 = beta_1\n","    self.epsilon = epsilon"],"metadata":{"id":"vW_-Lk1ZXUlT","executionInfo":{"status":"ok","timestamp":1644620353833,"user_tz":420,"elapsed":16,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class Layer: # Layer class stores a single layer of the network and performs forward and backward propagation\n","  def __init__(self, activation = None, activation_prime = None, prev_layer = None, num_inputs = 0, num_nodes = None, eta = .01, batch_normalize = True):\n","    ###\n","    # Initialize layer class\n","    #\n","    # Parameters:\n","    # activation- Chosen activation function. One of the presets created in Activation_Functions.ipynb should be passed here \n","    # activation_prime- Derivative of chosen activation function. One of the presets created in Activation_Functions.ipynb should be passed here \n","    # prev_layer- Previous layer in network\n","    # num_inputs- Number of nodes in the previous layer, of number of features per sample if this is the input layer\n","    # num_nodes- Number of nodes in this layer\n","    # eta- Learning rate\n","    # batch_normalize- Perform batch normalization if true\n","    ### \n","\n","    self.activation = activation\n","    self.activation_prime = activation_prime\n","\n","    self.initialization_strategy = None\n","    self.set_initialization_strategy()\n","\n","    self.weights = self.initialization_strategy(num_nodes, num_inputs)\n","    self.biases = np.zeros(num_nodes)\n","\n","    self.num_nodes = num_nodes\n","    self.num_inputs = num_inputs\n","\n","    self.inputs = []\n","    self.z = []\n","    self.a = []\n","    self.predictions = []\n","\n","    self.error_out = []\n","    self.weights_gradient = []\n","    self.biases_gradient = []\n","\n","    self.next_layer = None\n","    self.prev_layer = prev_layer\n","\n","    self.eta = eta\n","    self.alpha = 1\n","    self.adam_parameters = Adam_Parameters(self)\n","\n","  def set_initialization_strategy(self): \n","    ###\n","    # Choose weight initialization strategy based on this layer's activation function\n","    ###\n","\n","    if(self.activation == None):\n","      self.initialization_strategy = none\n","    elif(self.activation == ReLU):\n","      self.initialization_strategy = He\n","    else:\n","      self.initialization_strategy = Xavier\n","    \n","  def get_output_layer(self):\n","    ###\n","    # Find output layer given the input layer\n","    ###\n","\n","    if(self.next_layer == None):\n","      return self\n","    return self.next_layer.get_output_layer()\n","\n","  def get_input_layer(self):\n","    ###\n","    # Find input layer given the output layer\n","    ###\n","\n","    if(self.prev_layer == None):\n","      return self\n","    return self.prev_layer.get_input_layer()\n","\n","  def batch_forward_propagate(self, X, store_data = True):\n","    ###\n","    # Function to perform forward propagation for a given batch\n","    #\n","    # Parameters:\n","    # X- Samples in current batch\n","    # store_data- Whether or not to store values for backpropagation. Will be false when testing, true when training\n","    ###\n","\n","    if(self.prev_layer != None): # Ensure that function was called on the correct layer\n","      raise Exception(\"batch_forward_propagate() can only be be called on input layer\")\n","\n","    def forward_propagate(layer, X_i, store_data = True):\n","      ###\n","      # Function to forward propagate a single sample\n","      #\n","      # Parameters:\n","      # layer- The current layer\n","      # X_i- single sample from batch\n","      # store_data- described above\n","      ###\n","\n","      inputs = np.asarray(X_i)\n","        \n","      if(layer.prev_layer == None): # Check if the current layer is the input layer\n","        z = inputs\n","        a = z\n","          \n","      else: # Otherwise, calculate z and a values\n","        z = np.dot(layer.weights, inputs) + layer.biases\n","        a = layer.activation(z)\n","\n","      if(store_data): # Store values if needed for back propagation\n","        layer.inputs.append(inputs)\n","        layer.z.append(z)\n","        layer.a.append(a)\n","\n","      if(layer.next_layer != None): # Check if current layer is output layer, otherwise recursively call forward_propagate on the next layer\n","        forward_propagate(layer.next_layer, a, store_data = store_data)\n","\n","      layer.predictions.append(a)\n","\n","    current_layer = self\n","    while(current_layer != None): # Reset layer values\n","      current_layer.inputs = []\n","      current_layer.z = []\n","      current_layer.a = []\n","      current_layer.error_out = []\n","      current_layer = current_layer.next_layer\n","\n","    for sample in np.atleast_2d(X): # Call forward_propagate\n","      forward_propagate(self, sample, store_data)\n","\n","    output_layer = self.get_output_layer()\n","    if(not store_data): # If store_data is false, final predictions are stored\n","      output = output_layer.predictions[-np.shape(X)[0]:] # Remove last output of network so it won't be included in backpropagation\n","      return output\n","\n","    return np.asarray(output_layer.a) # Return a values of output layer\n","\n","  def batch_backward_propagate(self, X, y, epoch_count):\n","    ###\n","    # Function to perform backward propagation for a given batch\n","    #\n","    # Parameters:\n","    # X- Samples in current batch\n","    # y- Labels in current batch\n","    # epoch_count- current epoch\n","    ###\n","\n","    if(self.next_layer != None): # Ensure that function was called on the correct layer\n","      raise Exception(\"batch_backward_propagate() can only be be called on output layer\")\n","\n","    num_samples = np.shape(X)[0]\n","    indices = np.arange(num_samples)\n","\n","    input_layer = self.get_input_layer()\n","    a = input_layer.batch_forward_propagate(X, store_data = True).flatten() # Forward propagte the batch\n","    y = np.asarray(y)\n","    a = np.reshape(a, np.shape(y))\n","    deltas = a - y # Calculate output layer errors\n","    \n","    if(epoch_count%100 == 0):\n","      print(\"epoch\", epoch_count, \"err:\", mse(y, a), \"derErr:\", sum(a-y)/len(y))\n","\n","    def backward_propagate(layer, error, index):\n","      ###\n","      # Function to find errors for a single sample\n","      #\n","      # Parameters:\n","      # layer- The current layer\n","      # error- Error from the following layer\n","      # index- Index of the individual sample\n","      ###\n","\n","      if(layer.next_layer == None): # Check if output layer\n","        new_error = np.asarray(error).flatten()\n","        layer.error_out.append([new_error])\n","\n","      elif(layer.prev_layer != None):\n","        coefficients = np.asarray(layer.next_layer.weights).T # Transposition of the next layer's weights\n","        errors = []\n","        new_error = np.zeros(np.shape(layer.next_layer.weights)[1])\n","\n","        for row in coefficients:\n","          error_to_add = np.dot(error, row) * layer.activation_prime(layer.z[index]) # Dot product of next layer error and weights times derivative of current layer z values\n","          errors.append(error_to_add)\n","          new_error = new_error + error_to_add\n","\n","        layer.error_out.append(errors)\n","\n","      if(layer.prev_layer != None): # Recursively call backward_propagate on the previous layer\n","        backward_propagate(layer.prev_layer, new_error, index)\n","\n","    def compute_gradients(layer, divisor):\n","      ###\n","      # Function to compute gradients of a layer based on its error\n","      #\n","      # Parameters:\n","      # layer- current layer\n","      # divisor- numbber of samples in batch for averaging\n","      ###\n","\n","      if(layer.prev_layer != None): # Ensure current layer is not input\n","        weights_gradient = np.zeros(np.shape(layer.weights))\n","        final_deltas = []\n","\n","        for delta, current_input, current_z in zip(layer.error_out, layer.inputs, layer.z):\n","          delta_tmp = []\n","          for lis in delta: # This loop is a temporary fix for some numpy issues, will be removed later\n","            delta_tmp.append(np.asarray(lis.flatten()))\n","            final_deltas.append(np.asarray(lis.flatten()))\n","          delta = np.asarray(delta_tmp)\n","          \n","          for err in delta: # Calculate gradient for each error\n","            grad = np.dot(current_input[np.newaxis].T, np.asarray(err)[np.newaxis])\n","            weights_gradient = weights_gradient + grad.T\n","        \n","        weights_gradient = weights_gradient\n","        #weights_gradient /= divisor\n","\n","        biases_gradient = np.mean(np.asarray(final_deltas), 0)\n","\n","        layer.weights_gradient = weights_gradient # Set gradients for instance of layer class\n","        layer.biases_gradient = biases_gradient\n","\n","        compute_gradients(layer.prev_layer, divisor) # Recursive call on previous layer\n","\n","    \n","    for delta, sample_index in zip(np.atleast_2d(deltas), indices): # Backpropagate for each sample\n","      backward_propagate(self, delta, sample_index)\n","\n","    compute_gradients(self, num_samples)\n","    \n","\n","  \n","  def update_parameters(self): # Update weights and biases for the layer\n","    if(self.prev_layer != None):\n","      self.prev_layer.update_parameters()\n","      self.weights = self.weights - (self.weights_gradient * self.eta)\n","      self.biases = self.biases - (self.biases_gradient * self.eta) \n"],"metadata":{"id":"spQaskVz-4qB","executionInfo":{"status":"ok","timestamp":1644620476678,"user_tz":420,"elapsed":519,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Network:\n","  def __init__(self, solver = Adam_solver, activation = ReLU, activation_prime = ReLU_prime, eta = .01, batch_proportion = .25):\n","    ###\n","    # Initialize neural net\n","    #\n","    # Parameters:\n","    # solver- chosen solver\n","    # activation- Chosen activation function. One of the presets created in Activation_Functions.ipynb should be passed here \n","    # activation_prime- Derivative of chosen activation function. One of the presets created in Activation_Functions.ipynb should be passed here \n","    # eta- Learning rate\n","    # batch_proportion- Value to calculate size of each batch\n","    ###\n","\n","    self.solver = solver\n","    self.activation = activation\n","    self.activation_prime = activation_prime\n","\n","    self.input_layer = []\n","    self.output_layer = []\n","    self.layer_dimensions = []   \n","    \n","    self.batch_proportion = batch_proportion\n","    self.eta = eta\n","    \n","  def create_network(self, layer_dimensions):\n","    ###\n","    # Function to create the layers of the network\n","    #\n","    # Parameters:\n","    # layer_dimensions- List storing the dimensions of the network. The first element of the list\n","    # will store the number of features per sample of the training data, the final element stores the\n","    # number of possible class labels to assign. Elements of the list between the first and last will\n","    # be the desired number of nodes in each layer of the network.\n","    ###\n","\n","    self.layer_dimensions = layer_dimensions\n","    length = len(self.layer_dimensions)\n","    new_layer = None\n","    prev_layer = None\n","\n","    self.input_layer = Layer(eta = None, num_inputs = layer_dimensions[0])\n","    prev_layer = self.input_layer\n","\n","    for index in range(length-1):\n","\n","        new_layer = Layer(prev_layer = prev_layer,\n","                          num_inputs = self.layer_dimensions[index],\n","                          num_nodes = self.layer_dimensions[index+1],\n","                          activation = self.activation,\n","                          activation_prime = self.activation_prime)\n","        \n","        new_layer.prev_layer.next_layer = new_layer\n","        prev_layer = new_layer\n","\n","    self.output_layer = prev_layer\n","    self.output_layer.activation = softmax # Output layer uses softmax activation\n","    self.output_layer.activation_prime = identity_prime # No need to program softmax_prime, it cancels out with the correct loss function\n","\n","  def predict(self, X):\n","    ###\n","    # Function to predict class labels\n","    #\n","    # Parameters:\n","    # X- List of samples to predict\n","    ###\n","\n","    predictions = self.input_layer.batch_forward_propagate(X, store_data = False)\n","    predictions_tmp = []\n","    for lis in predictions:\n","      predictions_tmp.append(np.argmax(np.asarray(lis.flatten())))\n","    predictions = np.asarray(predictions_tmp)\n","    return predictions\n","\n","  def fit(self, X, y, epochs):\n","    ###\n","    # Simply calls the chosen solver\n","    #\n","    # Parameters:\n","    # X- Training data features\n","    # y- Training data labels\n","    # epochs- Number of epochs to run\n","    ###\n","    \n","    self.solver(self.input_layer, X, y, epochs, self.batch_proportion)"],"metadata":{"id":"ofUZwczNIdt1","executionInfo":{"status":"ok","timestamp":1644620480682,"user_tz":420,"elapsed":165,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# This block of code simply tests the network on performing an exclusive-or operation for simple and quick testing\n","\n","X = np.array([[0,0], [0,1], [1,0], [1,1]])\n","y = np.array([[1,0], [0,1], [0,1], [1,0]])\n","y_argmax = np.array([np.argmax(y_i) for y_i in y])\n","\n","xorNetwork = Network(eta = .1, solver = Adamax_solver, activation = ReLU, activation_prime = ReLU_prime, batch_proportion = 1)\n","xorNetwork.create_network([2, 10, 2])\n","\n","xorNetwork.fit(X, y, 2000)\n","\n","preds = xorNetwork.predict(X)\n","print(\"Predictions:\\n\",preds,\"\\nactual:\\n\", y_argmax) # Print the class labels as predicted by the network and the actual class labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmryUNG_I8uv","executionInfo":{"status":"ok","timestamp":1644620486122,"user_tz":420,"elapsed":2635,"user":{"displayName":"Samuel Britten","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07377026485778167557"}},"outputId":"dbc274fb-d086-44c5-cb95-80abdbc2bbc8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0 err: 0.3223023617095754 derErr: [ 0.30836182 -0.30836182]\n","epoch 100 err: 0.24541987414321287 derErr: [ 0.05209539 -0.05209539]\n","epoch 200 err: 0.23568312463691046 derErr: [-0.25388157  0.25388157]\n","epoch 300 err: 0.16834035328408298 derErr: [ 0.28837905 -0.28837905]\n","epoch 400 err: 0.0902718083442398 derErr: [ 0.1480076 -0.1480076]\n","epoch 500 err: 0.1737977596142922 derErr: [ 0.05276686 -0.05276686]\n","epoch 600 err: 0.15339107744575128 derErr: [-0.30049791  0.30049791]\n","epoch 700 err: 0.2071445900476923 derErr: [-0.4551314  0.4551314]\n","epoch 800 err: 0.06140048876755964 derErr: [ 0.05195484 -0.05195484]\n","epoch 900 err: 0.0345885690670213 derErr: [ 0.05576657 -0.05576657]\n","epoch 1000 err: 0.027737763654773186 derErr: [ 0.1022093 -0.1022093]\n","epoch 1100 err: 0.008954868346945123 derErr: [-0.01454362  0.01454362]\n","epoch 1200 err: 0.0075428266192705835 derErr: [ 0.03813434 -0.03813434]\n","epoch 1300 err: 0.003164060660605318 derErr: [-0.00332878  0.00332878]\n","epoch 1400 err: 0.0033454641647577225 derErr: [ 0.0192867 -0.0192867]\n","epoch 1500 err: 0.0011671344295330925 derErr: [ 0.00349624 -0.00349624]\n","epoch 1600 err: 0.000261695902115405 derErr: [-0.01491724  0.01491724]\n","epoch 1700 err: 0.0009763385989573544 derErr: [ 0.01478398 -0.01478398]\n","epoch 1800 err: 0.00039468995758516187 derErr: [ 0.00256996 -0.00256996]\n","epoch 1900 err: 8.376278403129653e-05 derErr: [-0.00849839  0.00849839]\n","Predictions:\n"," [0 1 1 0] \n","actual:\n"," [0 1 1 0]\n"]}]}]}